<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="Sultan Alrashed - Research Engineer at KAUST, focused on LLM pretraining and Arabic NLP">
  <title>Sultan Alrashed</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,wght@0,400;0,500;0,600;1,400&family=Fraunces:ital,opsz,wght@0,9..144,400;0,9..144,600;1,9..144,400&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="container">

    <!-- Header -->
    <header class="cv-header">
      <h1>Sultan Alrashed</h1>
      <div class="cv-intro">
        <img src="headshot.JPG" alt="Sultan Alrashed" class="headshot">
        <div class="intro-text">
          <p>
            Research engineer at KAUST (in Prof. Francesco Orabona's lab) working on pretraining bilingual Arabic-English LLMs.
            I've spent the last few years getting my hands dirty with large-scale distributed training! Most recent thing I've done is release a SOTA Arabic pretraining dataset (AraMix).
          </p>
          <p>
            Previously at SDAIA as a founding member of the ALLaM team, where I helped build Saudi Arabia's
            flagship Arabic language model.
          </p>
        </div>
        <div class="contact-info">
          <a href="https://linkedin.com/in/sulrash">linkedin.com/in/sulrash</a>
          <span class="separator">•</span>
          <a href="https://github.com/sulrash">github.com/sulrash</a>
          <span class="separator">•</span>
          <a href="https://huggingface.co/SultanR">huggingface.co/SultanR</a>
        </div>
      </div>
    </header>

      <!-- Experience -->
      <details class="cv-section" open>
        <summary><h2>Experience</h2></summary>
  
        <div class="entry">
          <div class="entry-header">
            <span class="entry-title">Research Engineer</span>
            <span class="entry-date">Jan. 2025 — Current</span>
          </div>
          <div class="entry-subtitle">
            <span class="entry-org">King Abdullah University of Science & Technology (KAUST)</span>
            <span class="entry-location">Thuwal, SA</span>
          </div>
          <p class="entry-desc">Research engineer in the <a href="https://sites.google.com/view/optimal-lab/home">OPTIMAL lab</a> under <a href="https://francesco.orabona.com">Prof. Francesco Orabona</a>. Leading development of a bilingual Arabic-English LLM at the 3B parameter and 5T token scale.</p>
        </div>
  
        <div class="entry">
          <div class="entry-header">
            <span class="entry-title">Artificial Intelligence Engineer</span>
            <span class="entry-date">Jan. 2023 — Jan. 2025</span>
          </div>
          <div class="entry-subtitle">
            <span class="entry-org">Saudi Data & Artificial Intelligence Authority (SDAIA)</span>
            <span class="entry-location">Riyadh, SA</span>
          </div>
          <p class="entry-desc">Founding member of the ALLaM team, worked across the full LLM pipeline because of our initial understaffing.</p>
        </div>
  
        <div class="entry">
          <div class="entry-header">
            <span class="entry-title">Research Engineering Fellowship</span>
            <span class="entry-date">Apr. 2024 — Oct. 2024</span>
          </div>
          <div class="entry-subtitle">
            <span class="entry-org">KAUST & SDAIA Partnership</span>
            <span class="entry-location">Thuwal, SA</span>
          </div>
          <p class="entry-desc">Selected for a fellowship program where I focused on AI for education. Built an AI-based learning management system for the Ministry of Education that I presented to the Minister at GAIN 2024. System started piloting in public schools.</p>
        </div>
      </details>

    <!-- Publications -->
    <details class="cv-section" open>
      <summary><h2>Publications & Preprints</h2></summary>

      <div class="publication">
        <span class="pub-tag">First Author</span>
        <span class="pub-venue">EACL Findings 2026</span><br>
        <a href="https://openreview.net/forum?id=0HF2Dg0Ldx" class="pub-title">Cards Against Contamination: TCG-Bench for Difficulty-Scalable Multilingual LLM Reasoning</a>
        <div class="pub-desc">A contamination-resistant English/Arabic text-game suite with scalable difficulty.</div>
      </div>

      <div class="publication">
        <span class="pub-tag">First Author</span>
        <span class="pub-venue">Arxiv 2025</span><br>
        <a href="https://huggingface.co/datasets/AdaMLLab/AraMix" class="pub-title">AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus</a>
        <div class="pub-desc">Introduces the largest and best performing Arabic pretraining dataset at 178B tokens. Uses novel cross-dataset agreement metric as a quality signal.</div>
      </div>

      <div class="publication">
        <span class="pub-tag">First Author</span>
        <span class="pub-venue">Arxiv 2025</span><br>
        <a href="https://arxiv.org/abs/2511.18411" class="pub-title">SmolKalam: Ensemble Quality-Filtered Translation at Scale for High Quality Arabic Post-Training Data</a>
        <div class="pub-desc">Introduces the largest multi-turn, tool-calling, reasoning-inclusive Arabic post-training dataset, at around 2B tokens.</div>
      </div>

      <div class="publication">
        <span class="pub-tag">Co-Author</span>
        <span class="pub-venue">Arxiv 2025</span><br>
        <a href="https://arxiv.org/abs/2510.19933" class="pub-title">Beyond the Ideal: Analyzing the Inexact Muon Update</a>
        <div class="pub-desc">Analysis of Muon's inexact orthogonalized update with performance bounds under an additive-error LMO framework.</div>
      </div>

      <div class="publication">
        <span class="pub-tag">Co-Author</span>
        <span class="pub-venue">Arxiv 2025</span><br>
        <a href="https://arxiv.org/abs/2510.24081" class="pub-title">Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures</a>
        <div class="pub-desc">Wrote the Arabic Saudi dialect portion of a multilingual version of the PIQA benchmark.</div>
      </div>

      <div class="publication">
        <span class="pub-tag">Co-Author</span>
        <span class="pub-venue">ICML 2025 World Models Workshop</span><br>
        <a href="https://openreview.net/forum?id=Z4KBiAYXlI" class="pub-title">ReviseQA: A Benchmark for Belief Revision in Multi-Turn Logical Reasoning</a>
        <div class="pub-desc">Benchmark testing logical consistency under iterative context updates.</div>
      </div>

      <div class="publication">
        <span class="pub-tag">First Author</span>
        <span class="pub-venue">Arxiv 2024</span><br>
        <a href="https://arxiv.org/abs/2411.06402" class="pub-title">Fineweb-Edu-Ar: Machine-translated Corpus to Support Arabic Small Language Models</a>
        <div class="pub-desc">Introduces a machine-translated Arabic corpus derived from FineWeb-Edu (202B tokens) for training and evaluating Arabic language models, the largest at the time.</div>
      </div>

      <div class="publication">
        <span class="pub-tag">Solo Author</span>
        <span class="pub-venue">Arxiv 2024</span><br>
        <a href="https://arxiv.org/abs/2412.08347" class="pub-title">SmolTulu: Higher Learning Rate to Batch Size Ratios Can Lead to Better Reasoning in SLMs</a>
        <div class="pub-desc">Shows that higher LR:batch-size ratios can boost reasoning in small LMs. Achieved the highest IFEval score of any sub-3B model at release.</div>
      </div>

      <div class="publication">
        <span class="pub-tag">Co-Author</span>
        <span class="pub-venue">ICLR 2025</span><br>
        <a href="https://arxiv.org/abs/2407.15390" class="pub-title">ALLaM: A Series of Large Language Models for Arabic and English</a>
        <div class="pub-desc">SDAIA's bilingual Arabic/English pretrained LLM series. Initially was on pretraining, later focused on finetuning and alignment as the team scaled.</div>
      </div>

      <div class="publication">
        <span class="pub-tag">Co-Author</span>
        <span class="pub-venue">ACL Main Conference 2024</span><br>
        <a href="https://arxiv.org/abs/2402.01781" class="pub-title">When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards</a>
        <div class="pub-desc">A deep dive into the sensitivity of LLM benchmarks and evaluations to minor structural perturbations, many results contributed to LM-Harness.</div>
      </div>
    </details>

    <!-- Projects -->
    <details class="cv-section" open>
      <summary><h2>Projects & Volunteering</h2></summary>

      <div class="project">
        <span class="project-name"><a href="https://github.com/pytorch/captum">PyTorch Captum Contributions</a></span>
        <span class="project-desc">Improved support for LLMs in PyTorch's Captum repository, adding support for a larger range of models and tasks.</span>
      </div>

      <div class="project">
        <span class="project-name"><a href="https://github.com/Microsoft/Megatron-DeepSpeed">Megatron-Deepspeed Contributions</a></span>
        <span class="project-desc">Fixed backwards compatibility bug.</span>
      </div>

      <div class="project">
        <span class="project-name"><a href="https://github.com/huggingface/lighteval">Lighteval Contributions</a></span>
        <span class="project-desc">Added quantization support for vLLM models.</span>
      </div>

      <div class="project">
        <span class="project-name"><a href="https://github.com/huggingface/nanotron">Nanotron Contributions</a></span>
        <span class="project-desc">Fixed bugs to get pretraining example in docs to work.</span>
      </div>

      <div class="project">
        <span class="project-name"><a href="https://github.com/SulRash/ntaGPT">Next-Token Agent</a></span>
        <span class="project-desc">A project focused on pretraining and finetuning tiny language models to solve ASCII games by predicting each successive frame, able to perfectly solve mazes from (<code>frozenlake</code>).</span>
      </div>

      <div class="project">
        <span class="project-name"><a href="https://github.com/SulRash/envenc">Environment Encoder</a></span>
        <span class="project-desc">A proposal and implementation of an idea to train a reinforcement learning agent to play games given a vision-language model's embeddings, instead of frames.</span>
      </div>

      <div class="project">
        <span class="project-name"><a href="https://github.com/SulRash/Graduation-Project">Reinforcement Learning Roguelike Solver</a></span>
        <span class="project-desc">For my university honours project, I wrote PPO, DQN, and A2C agents to compare effects of perfect and imperfect information on my CLI game.</span>
      </div>

      <div class="project">
        <span class="project-name"><a href="https://github.com/SulRash/Cheatsheet">Cheatsheet</a></span>
        <span class="project-desc">A never finished project that implements a new augmentation and objective for learning classification for computer vision models.</span>
      </div>
    </details>

  </div>
</body>
</html>
